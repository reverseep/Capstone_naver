{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "x_train = Variable(torch.tensor([[1],[2],[3]], dtype = torch.float32))\n",
    "y_train = Variable(torch.tensor([[2.1],[3.1],[4.1]], dtype = torch.float32))\n",
    "\n",
    "cost = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "model = LinearRegression(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n",
      "tensor([[0.2048]])\n",
      "tensor([-0.4042])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2001):\n",
    "    pred = model(x_train)\n",
    "    loss = cost(pred, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        for param in model.parameters():\n",
    "            print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7122961\n",
      "500 0.46825695\n",
      "1000 0.4095371\n",
      "1500 0.37381706\n",
      "2000 0.3448322\n",
      "2500 0.3197251\n",
      "3000 0.29761967\n",
      "3500 0.27804324\n",
      "4000 0.2606411\n",
      "4500 0.24511647\n",
      "5000 0.23121582\n",
      "5500 0.21872222\n",
      "6000 0.20745058\n",
      "6500 0.19724338\n",
      "7000 0.18796611\n",
      "7500 0.17950457\n",
      "8000 0.17176092\n",
      "8500 0.16465135\n",
      "9000 0.15810402\n",
      "9500 0.15205695\n",
      "10000 0.14645647\n",
      "\n",
      "Hypothesis:  [[0.02947675]\n",
      " [0.15709856]\n",
      " [0.29876292]\n",
      " [0.7841844 ]\n",
      " [0.9413274 ]\n",
      " [0.9807585 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [1], [1], [1]], dtype=np.float32)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "linear = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nHypothesis: \", hypothesis.data.numpy(), \"\\nCorrect (Y): \", predicted.numpy(), \"\\nAccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "0 0.67878276\n",
      "500 0.6063051\n",
      "1000 0.57939583\n",
      "1500 0.55945337\n",
      "2000 0.5442791\n",
      "2500 0.5325177\n",
      "3000 0.5232431\n",
      "3500 0.5158152\n",
      "4000 0.5097841\n",
      "4500 0.5048278\n",
      "5000 0.5007114\n",
      "5500 0.49726045\n",
      "6000 0.49434358\n",
      "6500 0.49186006\n",
      "7000 0.48973137\n",
      "7500 0.48789605\n",
      "8000 0.48630518\n",
      "8500 0.4849191\n",
      "9000 0.48370624\n",
      "9500 0.4826403\n",
      "10000 0.4816996\n",
      "\n",
      "Hypothesis:  [[0.4214937 ]\n",
      " [0.9252549 ]\n",
      " [0.22146834]\n",
      " [0.94033486]\n",
      " [0.20326968]\n",
      " [0.74872816]\n",
      " [0.93724126]\n",
      " [0.5906591 ]\n",
      " [0.25505206]\n",
      " [0.5218534 ]\n",
      " [0.6977034 ]\n",
      " [0.17590402]\n",
      " [0.2684236 ]\n",
      " [0.31107903]\n",
      " [0.7386324 ]\n",
      " [0.45764264]\n",
      " [0.7226729 ]\n",
      " [0.85679585]\n",
      " [0.81325054]\n",
      " [0.5882922 ]\n",
      " [0.66751385]\n",
      " [0.10722429]\n",
      " [0.648355  ]\n",
      " [0.6625307 ]\n",
      " [0.36500865]\n",
      " [0.93263626]\n",
      " [0.5356613 ]\n",
      " [0.6368532 ]\n",
      " [0.7149202 ]\n",
      " [0.44177476]\n",
      " [0.9487992 ]\n",
      " [0.8590053 ]\n",
      " [0.5862083 ]\n",
      " [0.8241071 ]\n",
      " [0.3632261 ]\n",
      " [0.6525909 ]\n",
      " [0.82764024]\n",
      " [0.564164  ]\n",
      " [0.44598982]\n",
      " [0.37463152]\n",
      " [0.8268522 ]\n",
      " [0.1609929 ]\n",
      " [0.4000154 ]\n",
      " [0.06932937]\n",
      " [0.5834685 ]\n",
      " [0.93226326]\n",
      " [0.7150622 ]\n",
      " [0.71118814]\n",
      " [0.93305284]\n",
      " [0.93039644]\n",
      " [0.9260626 ]\n",
      " [0.23517005]\n",
      " [0.36229932]\n",
      " [0.9657466 ]\n",
      " [0.20823754]\n",
      " [0.47930914]\n",
      " [0.14636691]\n",
      " [0.7013391 ]\n",
      " [0.87342167]\n",
      " [0.49901906]\n",
      " [0.94787943]\n",
      " [0.70476705]\n",
      " [0.65838933]\n",
      " [0.8494433 ]\n",
      " [0.60518634]\n",
      " [0.5963609 ]\n",
      " [0.9533247 ]\n",
      " [0.6754966 ]\n",
      " [0.8567422 ]\n",
      " [0.6571179 ]\n",
      " [0.27096242]\n",
      " [0.7142837 ]\n",
      " [0.9179022 ]\n",
      " [0.9268403 ]\n",
      " [0.8776257 ]\n",
      " [0.7929408 ]\n",
      " [0.42279813]\n",
      " [0.86536235]\n",
      " [0.8968044 ]\n",
      " [0.9165874 ]\n",
      " [0.8625472 ]\n",
      " [0.8197806 ]\n",
      " [0.36837217]\n",
      " [0.812143  ]\n",
      " [0.5265084 ]\n",
      " [0.86820215]\n",
      " [0.40271515]\n",
      " [0.897364  ]\n",
      " [0.93929785]\n",
      " [0.768381  ]\n",
      " [0.802375  ]\n",
      " [0.6595824 ]\n",
      " [0.71746916]\n",
      " [0.5743555 ]\n",
      " [0.90262544]\n",
      " [0.9747192 ]\n",
      " [0.8862763 ]\n",
      " [0.5970791 ]\n",
      " [0.2686458 ]\n",
      " [0.6422722 ]\n",
      " [0.60745347]\n",
      " [0.9563265 ]\n",
      " [0.78311217]\n",
      " [0.76911974]\n",
      " [0.8758323 ]\n",
      " [0.68108326]\n",
      " [0.91786253]\n",
      " [0.8068218 ]\n",
      " [0.4806655 ]\n",
      " [0.3657375 ]\n",
      " [0.9260479 ]\n",
      " [0.87254226]\n",
      " [0.42893052]\n",
      " [0.4466778 ]\n",
      " [0.62904394]\n",
      " [0.8410254 ]\n",
      " [0.8683263 ]\n",
      " [0.91851044]\n",
      " [0.14341591]\n",
      " [0.72403395]\n",
      " [0.847055  ]\n",
      " [0.62615204]\n",
      " [0.620998  ]\n",
      " [0.81861186]\n",
      " [0.70069796]\n",
      " [0.8329596 ]\n",
      " [0.8105561 ]\n",
      " [0.6232568 ]\n",
      " [0.5035219 ]\n",
      " [0.42663676]\n",
      " [0.42695493]\n",
      " [0.78037935]\n",
      " [0.93123853]\n",
      " [0.82706946]\n",
      " [0.7853238 ]\n",
      " [0.844472  ]\n",
      " [0.4537532 ]\n",
      " [0.79559296]\n",
      " [0.7343338 ]\n",
      " [0.73139894]\n",
      " [0.8766543 ]\n",
      " [0.6300091 ]\n",
      " [0.5627845 ]\n",
      " [0.71061796]\n",
      " [0.9053527 ]\n",
      " [0.75637233]\n",
      " [0.45883697]\n",
      " [0.93003625]\n",
      " [0.6276516 ]\n",
      " [0.7785105 ]\n",
      " [0.2698899 ]\n",
      " [0.4019054 ]\n",
      " [0.11513749]\n",
      " [0.24784109]\n",
      " [0.9140518 ]\n",
      " [0.8759011 ]\n",
      " [0.9376869 ]\n",
      " [0.11231991]\n",
      " [0.5181308 ]\n",
      " [0.7650791 ]\n",
      " [0.6021612 ]\n",
      " [0.8726909 ]\n",
      " [0.41984954]\n",
      " [0.8038901 ]\n",
      " [0.61725855]\n",
      " [0.64769816]\n",
      " [0.72673523]\n",
      " [0.85917515]\n",
      " [0.76064   ]\n",
      " [0.61848855]\n",
      " [0.89140344]\n",
      " [0.87833345]\n",
      " [0.94828564]\n",
      " [0.2216121 ]\n",
      " [0.815437  ]\n",
      " [0.24314453]\n",
      " [0.39522886]\n",
      " [0.41205373]\n",
      " [0.8741178 ]\n",
      " [0.6639248 ]\n",
      " [0.9204757 ]\n",
      " [0.90594405]\n",
      " [0.59235895]\n",
      " [0.15011212]\n",
      " [0.20427266]\n",
      " [0.60694915]\n",
      " [0.72434056]\n",
      " [0.6197645 ]\n",
      " [0.8313568 ]\n",
      " [0.6035478 ]\n",
      " [0.35921216]\n",
      " [0.22424865]\n",
      " [0.90009815]\n",
      " [0.37864023]\n",
      " [0.8680258 ]\n",
      " [0.8952472 ]\n",
      " [0.71755874]\n",
      " [0.6398187 ]\n",
      " [0.6355873 ]\n",
      " [0.5689977 ]\n",
      " [0.7162163 ]\n",
      " [0.94146377]\n",
      " [0.7661687 ]\n",
      " [0.8107887 ]\n",
      " [0.13871217]\n",
      " [0.31476653]\n",
      " [0.90477216]\n",
      " [0.21149144]\n",
      " [0.9344926 ]\n",
      " [0.27427834]\n",
      " [0.2580172 ]\n",
      " [0.46643424]\n",
      " [0.6931687 ]\n",
      " [0.21409316]\n",
      " [0.74764246]\n",
      " [0.7121468 ]\n",
      " [0.8179361 ]\n",
      " [0.673074  ]\n",
      " [0.16579153]\n",
      " [0.3673737 ]\n",
      " [0.7037    ]\n",
      " [0.5304073 ]\n",
      " [0.921605  ]\n",
      " [0.93125755]\n",
      " [0.6869721 ]\n",
      " [0.38547564]\n",
      " [0.04817293]\n",
      " [0.63683635]\n",
      " [0.3644061 ]\n",
      " [0.46170595]\n",
      " [0.94604504]\n",
      " [0.63902664]\n",
      " [0.94502246]\n",
      " [0.2345882 ]\n",
      " [0.14393498]\n",
      " [0.28390157]\n",
      " [0.7485196 ]\n",
      " [0.91662675]\n",
      " [0.8756109 ]\n",
      " [0.64959437]\n",
      " [0.67949164]\n",
      " [0.5866555 ]\n",
      " [0.1593915 ]\n",
      " [0.54737705]\n",
      " [0.14241311]\n",
      " [0.57384163]\n",
      " [0.8652779 ]\n",
      " [0.6727724 ]\n",
      " [0.6990394 ]\n",
      " [0.9452662 ]\n",
      " [0.80862886]\n",
      " [0.77190244]\n",
      " [0.7756393 ]\n",
      " [0.7755382 ]\n",
      " [0.8581347 ]\n",
      " [0.40723202]\n",
      " [0.40866932]\n",
      " [0.52923864]\n",
      " [0.82000285]\n",
      " [0.6644985 ]\n",
      " [0.68644977]\n",
      " [0.81329983]\n",
      " [0.33404684]\n",
      " [0.5168265 ]\n",
      " [0.62522346]\n",
      " [0.62649584]\n",
      " [0.43528414]\n",
      " [0.89946043]\n",
      " [0.75929105]\n",
      " [0.9257404 ]\n",
      " [0.5505258 ]\n",
      " [0.7721134 ]\n",
      " [0.81392825]\n",
      " [0.8117162 ]\n",
      " [0.6817562 ]\n",
      " [0.86564755]\n",
      " [0.35062778]\n",
      " [0.56935996]\n",
      " [0.6694744 ]\n",
      " [0.3552507 ]\n",
      " [0.81058294]\n",
      " [0.3034678 ]\n",
      " [0.61932296]\n",
      " [0.93244064]\n",
      " [0.7730697 ]\n",
      " [0.84270954]\n",
      " [0.6876962 ]\n",
      " [0.5077728 ]\n",
      " [0.6622185 ]\n",
      " [0.4109516 ]\n",
      " [0.46122602]\n",
      " [0.64353704]\n",
      " [0.6117675 ]\n",
      " [0.6338968 ]\n",
      " [0.6253892 ]\n",
      " [0.21533488]\n",
      " [0.6784088 ]\n",
      " [0.89790154]\n",
      " [0.49787664]\n",
      " [0.62018746]\n",
      " [0.7563278 ]\n",
      " [0.4568185 ]\n",
      " [0.70917565]\n",
      " [0.5125269 ]\n",
      " [0.71355516]\n",
      " [0.8975597 ]\n",
      " [0.6541638 ]\n",
      " [0.68709964]\n",
      " [0.8615398 ]\n",
      " [0.56931186]\n",
      " [0.851281  ]\n",
      " [0.93640214]\n",
      " [0.30484578]\n",
      " [0.780706  ]\n",
      " [0.24612005]\n",
      " [0.7750006 ]\n",
      " [0.8093566 ]\n",
      " [0.69376236]\n",
      " [0.3351983 ]\n",
      " [0.79358613]\n",
      " [0.72768164]\n",
      " [0.75176924]\n",
      " [0.18181211]\n",
      " [0.8091278 ]\n",
      " [0.83959794]\n",
      " [0.5898854 ]\n",
      " [0.9398097 ]\n",
      " [0.27447215]\n",
      " [0.6780261 ]\n",
      " [0.9456448 ]\n",
      " [0.21725835]\n",
      " [0.4885899 ]\n",
      " [0.6859659 ]\n",
      " [0.32696608]\n",
      " [0.1800828 ]\n",
      " [0.83938676]\n",
      " [0.9143405 ]\n",
      " [0.85975236]\n",
      " [0.61466646]\n",
      " [0.67504925]\n",
      " [0.562355  ]\n",
      " [0.77189255]\n",
      " [0.8095281 ]\n",
      " [0.9290613 ]\n",
      " [0.7446736 ]\n",
      " [0.76651853]\n",
      " [0.5790791 ]\n",
      " [0.93599737]\n",
      " [0.9391091 ]\n",
      " [0.74617946]\n",
      " [0.2671875 ]\n",
      " [0.7031577 ]\n",
      " [0.36557835]\n",
      " [0.74326235]\n",
      " [0.21459116]\n",
      " [0.25582117]\n",
      " [0.4306801 ]\n",
      " [0.6960506 ]\n",
      " [0.39564285]\n",
      " [0.56883216]\n",
      " [0.842053  ]\n",
      " [0.64440185]\n",
      " [0.8503573 ]\n",
      " [0.94208646]\n",
      " [0.7437576 ]\n",
      " [0.10833141]\n",
      " [0.49293286]\n",
      " [0.83987856]\n",
      " [0.85536945]\n",
      " [0.6887902 ]\n",
      " [0.28456768]\n",
      " [0.8677471 ]\n",
      " [0.89576066]\n",
      " [0.30923092]\n",
      " [0.6095522 ]\n",
      " [0.8397438 ]\n",
      " [0.83787227]\n",
      " [0.87435555]\n",
      " [0.90559185]\n",
      " [0.8660008 ]\n",
      " [0.9153026 ]\n",
      " [0.68916947]\n",
      " [0.6110169 ]\n",
      " [0.5579258 ]\n",
      " [0.83775264]\n",
      " [0.8788865 ]\n",
      " [0.24101117]\n",
      " [0.81708133]\n",
      " [0.8682664 ]\n",
      " [0.33783174]\n",
      " [0.6439909 ]\n",
      " [0.86723685]\n",
      " [0.54535806]\n",
      " [0.90922046]\n",
      " [0.2777137 ]\n",
      " [0.8326218 ]\n",
      " [0.60663927]\n",
      " [0.8738585 ]\n",
      " [0.36372435]\n",
      " [0.7121014 ]\n",
      " [0.72272635]\n",
      " [0.7812571 ]\n",
      " [0.10455298]\n",
      " [0.24737722]\n",
      " [0.6814325 ]\n",
      " [0.8187255 ]\n",
      " [0.48152596]\n",
      " [0.7806162 ]\n",
      " [0.49468705]\n",
      " [0.36333892]\n",
      " [0.855068  ]\n",
      " [0.46925002]\n",
      " [0.919107  ]\n",
      " [0.8070853 ]\n",
      " [0.6695831 ]\n",
      " [0.9172107 ]\n",
      " [0.6645511 ]\n",
      " [0.8091242 ]\n",
      " [0.33715594]\n",
      " [0.28041074]\n",
      " [0.7483762 ]\n",
      " [0.4359507 ]\n",
      " [0.44307098]\n",
      " [0.8949815 ]\n",
      " [0.8928423 ]\n",
      " [0.90922725]\n",
      " [0.94766647]\n",
      " [0.6832119 ]\n",
      " [0.89923006]\n",
      " [0.3725227 ]\n",
      " [0.3572977 ]\n",
      " [0.47800156]\n",
      " [0.9407361 ]\n",
      " [0.60089636]\n",
      " [0.16883425]\n",
      " [0.92660767]\n",
      " [0.807595  ]\n",
      " [0.58474565]\n",
      " [0.80820084]\n",
      " [0.02485355]\n",
      " [0.9172515 ]\n",
      " [0.7540299 ]\n",
      " [0.7430159 ]\n",
      " [0.7472747 ]\n",
      " [0.9621199 ]\n",
      " [0.63782704]\n",
      " [0.7702598 ]\n",
      " [0.74813145]\n",
      " [0.8575674 ]\n",
      " [0.19020623]\n",
      " [0.65080214]\n",
      " [0.90253925]\n",
      " [0.6000131 ]\n",
      " [0.7348314 ]\n",
      " [0.94250274]\n",
      " [0.8410503 ]\n",
      " [0.87927353]\n",
      " [0.50587016]\n",
      " [0.77832836]\n",
      " [0.9373118 ]\n",
      " [0.74228144]\n",
      " [0.6397956 ]\n",
      " [0.31653175]\n",
      " [0.4858331 ]\n",
      " [0.5206036 ]\n",
      " [0.61709017]\n",
      " [0.5162482 ]\n",
      " [0.7734165 ]\n",
      " [0.5695532 ]\n",
      " [0.77827185]\n",
      " [0.8160064 ]\n",
      " [0.7159251 ]\n",
      " [0.65538245]\n",
      " [0.4979031 ]\n",
      " [0.5746044 ]\n",
      " [0.93170965]\n",
      " [0.8299146 ]\n",
      " [0.27129966]\n",
      " [0.43705696]\n",
      " [0.5358944 ]\n",
      " [0.12108008]\n",
      " [0.88528776]\n",
      " [0.15189882]\n",
      " [0.9000903 ]\n",
      " [0.8753428 ]\n",
      " [0.83268875]\n",
      " [0.69308555]\n",
      " [0.8892382 ]\n",
      " [0.36156347]\n",
      " [0.7701609 ]\n",
      " [0.9365153 ]\n",
      " [0.29661927]\n",
      " [0.43911883]\n",
      " [0.86471355]\n",
      " [0.8749413 ]\n",
      " [0.6721448 ]\n",
      " [0.8115804 ]\n",
      " [0.81560063]\n",
      " [0.79567236]\n",
      " [0.26100776]\n",
      " [0.7715917 ]\n",
      " [0.9054374 ]\n",
      " [0.61986613]\n",
      " [0.78544766]\n",
      " [0.69739485]\n",
      " [0.80554914]\n",
      " [0.8664815 ]\n",
      " [0.9271555 ]\n",
      " [0.59499407]\n",
      " [0.40732628]\n",
      " [0.76866513]\n",
      " [0.75219387]\n",
      " [0.96359485]\n",
      " [0.75542516]\n",
      " [0.6950121 ]\n",
      " [0.4193302 ]\n",
      " [0.7169032 ]\n",
      " [0.92526084]\n",
      " [0.94721854]\n",
      " [0.8836314 ]\n",
      " [0.6890079 ]\n",
      " [0.6577052 ]\n",
      " [0.8091707 ]\n",
      " [0.4842625 ]\n",
      " [0.82090026]\n",
      " [0.79993665]\n",
      " [0.90015256]\n",
      " [0.6164671 ]\n",
      " [0.69130635]\n",
      " [0.8976081 ]\n",
      " [0.48616332]\n",
      " [0.53289455]\n",
      " [0.6619582 ]\n",
      " [0.7267285 ]\n",
      " [0.6664718 ]\n",
      " [0.895515  ]\n",
      " [0.9189012 ]\n",
      " [0.20126708]\n",
      " [0.15352184]\n",
      " [0.753318  ]\n",
      " [0.51646316]\n",
      " [0.23236673]\n",
      " [0.8431674 ]\n",
      " [0.9010949 ]\n",
      " [0.6920385 ]\n",
      " [0.9333046 ]\n",
      " [0.91671324]\n",
      " [0.74623233]\n",
      " [0.84288585]\n",
      " [0.68934995]\n",
      " [0.5673495 ]\n",
      " [0.7536044 ]\n",
      " [0.6077399 ]\n",
      " [0.12524849]\n",
      " [0.9009481 ]\n",
      " [0.87567735]\n",
      " [0.71063834]\n",
      " [0.91726923]\n",
      " [0.8652572 ]\n",
      " [0.8779402 ]\n",
      " [0.5780392 ]\n",
      " [0.6808947 ]\n",
      " [0.8853052 ]\n",
      " [0.71745837]\n",
      " [0.8503843 ]\n",
      " [0.90714633]\n",
      " [0.5966062 ]\n",
      " [0.8052083 ]\n",
      " [0.8312405 ]\n",
      " [0.5725605 ]\n",
      " [0.5147024 ]\n",
      " [0.10432681]\n",
      " [0.26504079]\n",
      " [0.8279708 ]\n",
      " [0.63268757]\n",
      " [0.6743043 ]\n",
      " [0.5721264 ]\n",
      " [0.93706924]\n",
      " [0.44034928]\n",
      " [0.8034636 ]\n",
      " [0.28835422]\n",
      " [0.8891578 ]\n",
      " [0.35793048]\n",
      " [0.7677484 ]\n",
      " [0.5815852 ]\n",
      " [0.8720119 ]\n",
      " [0.58571285]\n",
      " [0.23252966]\n",
      " [0.79242444]\n",
      " [0.94237685]\n",
      " [0.38790777]\n",
      " [0.9199923 ]\n",
      " [0.8667922 ]\n",
      " [0.8440241 ]\n",
      " [0.8066415 ]\n",
      " [0.4231266 ]\n",
      " [0.32326308]\n",
      " [0.7168378 ]\n",
      " [0.20173928]\n",
      " [0.94591075]\n",
      " [0.33718547]\n",
      " [0.9208146 ]\n",
      " [0.8785799 ]\n",
      " [0.43351513]\n",
      " [0.2106932 ]\n",
      " [0.7012036 ]\n",
      " [0.43451935]\n",
      " [0.82781756]\n",
      " [0.68613994]\n",
      " [0.9767952 ]\n",
      " [0.56304044]\n",
      " [0.6262886 ]\n",
      " [0.77457213]\n",
      " [0.81625104]\n",
      " [0.08338959]\n",
      " [0.7493534 ]\n",
      " [0.81020606]\n",
      " [0.83181655]\n",
      " [0.62593   ]\n",
      " [0.465811  ]\n",
      " [0.6002232 ]\n",
      " [0.90355545]\n",
      " [0.6373603 ]\n",
      " [0.75801927]\n",
      " [0.81514835]\n",
      " [0.84882295]\n",
      " [0.7905023 ]\n",
      " [0.56061924]\n",
      " [0.7868191 ]\n",
      " [0.8965139 ]\n",
      " [0.70383203]\n",
      " [0.95129555]\n",
      " [0.7829774 ]\n",
      " [0.6204603 ]\n",
      " [0.48453325]\n",
      " [0.8328064 ]\n",
      " [0.8447105 ]\n",
      " [0.48893586]\n",
      " [0.6492652 ]\n",
      " [0.24231724]\n",
      " [0.53603023]\n",
      " [0.80378217]\n",
      " [0.94466573]\n",
      " [0.83559656]\n",
      " [0.7145602 ]\n",
      " [0.76046884]\n",
      " [0.88197464]\n",
      " [0.52607054]\n",
      " [0.9241304 ]\n",
      " [0.5921911 ]\n",
      " [0.8473167 ]\n",
      " [0.30272183]\n",
      " [0.10194704]\n",
      " [0.2642399 ]\n",
      " [0.34431872]\n",
      " [0.7049428 ]\n",
      " [0.8178108 ]\n",
      " [0.6018089 ]\n",
      " [0.7317166 ]\n",
      " [0.812     ]\n",
      " [0.47937542]\n",
      " [0.3855375 ]\n",
      " [0.90789396]\n",
      " [0.8766898 ]\n",
      " [0.42099762]\n",
      " [0.6803473 ]\n",
      " [0.18665464]\n",
      " [0.37769374]\n",
      " [0.74116766]\n",
      " [0.7143262 ]\n",
      " [0.9011737 ]\n",
      " [0.9753459 ]\n",
      " [0.20372112]\n",
      " [0.71918476]\n",
      " [0.6093322 ]\n",
      " [0.4599316 ]\n",
      " [0.72343343]\n",
      " [0.7263304 ]\n",
      " [0.89663136]\n",
      " [0.72272885]\n",
      " [0.5032832 ]\n",
      " [0.648239  ]\n",
      " [0.15064874]\n",
      " [0.6875619 ]\n",
      " [0.5299508 ]\n",
      " [0.906115  ]\n",
      " [0.5602446 ]\n",
      " [0.55735683]\n",
      " [0.7821131 ]\n",
      " [0.71801937]\n",
      " [0.4955893 ]\n",
      " [0.75165564]\n",
      " [0.6496204 ]\n",
      " [0.3516104 ]\n",
      " [0.6254153 ]\n",
      " [0.8735681 ]\n",
      " [0.8368872 ]\n",
      " [0.5854876 ]\n",
      " [0.79394436]\n",
      " [0.28562614]\n",
      " [0.84930366]\n",
      " [0.6017336 ]\n",
      " [0.74308467]\n",
      " [0.42780197]\n",
      " [0.653488  ]\n",
      " [0.8296224 ]\n",
      " [0.18691984]\n",
      " [0.3051684 ]\n",
      " [0.8051051 ]\n",
      " [0.8168119 ]\n",
      " [0.7985594 ]\n",
      " [0.9034448 ]\n",
      " [0.79518425]\n",
      " [0.695399  ]\n",
      " [0.71980095]\n",
      " [0.7683936 ]\n",
      " [0.7018507 ]\n",
      " [0.792201  ]\n",
      " [0.48058817]\n",
      " [0.42976013]\n",
      " [0.8847914 ]\n",
      " [0.7845842 ]\n",
      " [0.6230007 ]\n",
      " [0.29065084]\n",
      " [0.8775562 ]\n",
      " [0.81672686]\n",
      " [0.838789  ]\n",
      " [0.6503896 ]\n",
      " [0.8854935 ]\n",
      " [0.87306684]\n",
      " [0.7813431 ]\n",
      " [0.4207031 ]\n",
      " [0.8930652 ]\n",
      " [0.90791154]\n",
      " [0.33006796]\n",
      " [0.16194156]\n",
      " [0.70443135]\n",
      " [0.41658834]\n",
      " [0.81239027]\n",
      " [0.33329397]\n",
      " [0.4487772 ]\n",
      " [0.45020285]\n",
      " [0.7664519 ]\n",
      " [0.86774766]\n",
      " [0.14385815]\n",
      " [0.37970492]\n",
      " [0.61487395]\n",
      " [0.49241033]\n",
      " [0.52059937]\n",
      " [0.7893754 ]\n",
      " [0.16373768]\n",
      " [0.91522527]\n",
      " [0.19665645]\n",
      " [0.8389584 ]\n",
      " [0.70952004]\n",
      " [0.7369032 ]\n",
      " [0.8159388 ]\n",
      " [0.7256185 ]\n",
      " [0.89527017]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  tensor(0.7668)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "# Hypothesis using sigmoid\n",
    "linear = torch.nn.Linear(8, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nHypothesis: \", hypothesis.data.numpy(), \"\\nCorrect (Y): \", predicted.numpy(), \"\\nAccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3687892\n",
      "200 0.5463318\n",
      "400 0.4515878\n",
      "600 0.3764263\n",
      "800 0.3045038\n",
      "1000 0.24127784\n",
      "1200 0.21791309\n",
      "1400 0.19881806\n",
      "1600 0.18269481\n",
      "1800 0.16890593\n",
      "2000 0.15698555\n",
      "--------------\n",
      "[[7.658726e-03 9.923315e-01 9.646330e-06]] [1]\n",
      "--------------\n",
      "[[0.82809925 0.15501598 0.0168848 ]] [0]\n",
      "--------------\n",
      "[[1.3566319e-08 3.5339023e-04 9.9964654e-01]] [2]\n",
      "--------------\n",
      "[[7.6587372e-03 9.9233150e-01 9.6463482e-06]\n",
      " [8.2809913e-01 1.5501611e-01 1.6884804e-02]\n",
      " [1.3566319e-08 3.5339023e-04 9.9964654e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],\n",
    "          [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "X = Variable(torch.Tensor(x_data))\n",
    "Y = Variable(torch.Tensor(y_data))\n",
    "nb_classes = 3\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "softmax = torch.nn.Softmax()\n",
    "linear = torch.nn.Linear(4, nb_classes, bias=True)\n",
    "model = torch.nn.Sequential(linear, softmax)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for step in range(2001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # Cross entropy cost/loss\n",
    "    cost = -Y * torch.log(hypothesis)\n",
    "    cost = torch.sum(cost, 1).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Testing & One-hot encoding\n",
    "print('--------------')\n",
    "a = model(Variable(torch.Tensor([[1, 11, 7, 9]])))\n",
    "print(a.data.numpy(), torch.max(a, 1)[1].data.numpy())\n",
    "\n",
    "print('--------------')\n",
    "b = model(Variable(torch.Tensor([[1, 3, 4, 3]])))\n",
    "print(b.data.numpy(), torch.max(b, 1)[1].data.numpy())\n",
    "\n",
    "print('--------------')\n",
    "c = model(Variable(torch.Tensor([[1, 1, 0, 1]])))\n",
    "print(c.data.numpy(), torch.max(c, 1)[1].data.numpy())\n",
    "\n",
    "print('--------------')\n",
    "all = model(Variable(torch.Tensor([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]])))\n",
    "print(all.data.numpy(), torch.max(all, 1)[1].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "one_hot tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.]])\n",
      "Step:     0\tLoss: 2.446\tAcc: 9.79%\n",
      "Step:   100\tLoss: 0.488\tAcc: 25.88%\n",
      "Step:   200\tLoss: 0.333\tAcc: 24.93%\n",
      "Step:   300\tLoss: 0.257\tAcc: 24.47%\n",
      "Step:   400\tLoss: 0.211\tAcc: 24.47%\n",
      "Step:   500\tLoss: 0.179\tAcc: 24.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   600\tLoss: 0.156\tAcc: 24.19%\n",
      "Step:   700\tLoss: 0.138\tAcc: 24.21%\n",
      "Step:   800\tLoss: 0.124\tAcc: 24.21%\n",
      "Step:   900\tLoss: 0.113\tAcc: 24.07%\n",
      "Step:  1000\tLoss: 0.103\tAcc: 24.07%\n",
      "Step:  1100\tLoss: 0.095\tAcc: 24.07%\n",
      "Step:  1200\tLoss: 0.088\tAcc: 24.07%\n",
      "Step:  1300\tLoss: 0.082\tAcc: 24.07%\n",
      "Step:  1400\tLoss: 0.077\tAcc: 24.07%\n",
      "Step:  1500\tLoss: 0.073\tAcc: 24.07%\n",
      "Step:  1600\tLoss: 0.069\tAcc: 24.07%\n",
      "Step:  1700\tLoss: 0.065\tAcc: 24.07%\n",
      "Step:  1800\tLoss: 0.062\tAcc: 24.07%\n",
      "Step:  1900\tLoss: 0.059\tAcc: 24.07%\n",
      "Step:  2000\tLoss: 0.056\tAcc: 24.07%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 4 True Y: 4\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 2 True Y: 2\n",
      "[True] Prediction: 3 True Y: 3\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 1 True Y: 1\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "# one hot encoding\n",
    "Y_one_hot = torch.zeros(Y.size()[0], nb_classes)\n",
    "Y_one_hot.scatter_(1, Y.long().data, 1)\n",
    "Y_one_hot = Variable(Y_one_hot)\n",
    "print(\"one_hot\", Y_one_hot.data)\n",
    "\n",
    "softmax = torch.nn.Softmax()\n",
    "model = torch.nn.Linear(16, nb_classes, bias=True)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for step in range(2001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # Label has to be 1D LongTensor\n",
    "    cost = criterion(hypothesis, Y.long().view(-1))\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    prediction = torch.max(softmax(hypothesis), 1)[1].float()\n",
    "\n",
    "    correct_prediction = (prediction.data == Y.data)\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, cost.data[0], accuracy))\n",
    "\n",
    "\n",
    "# Let's see if we can predict\n",
    "pred = torch.max(softmax(hypothesis), 1)[1].float()\n",
    "\n",
    "for p, y in zip(pred, Y):\n",
    "    print(\"[{}] Prediction: {} True Y: {}\".format(bool(p.data[0] == y.data[0]), p.data.int()[0], y.data.int()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.72739744\n",
      "500 0.6931795\n",
      "1000 0.6931476\n",
      "1500 0.6931472\n",
      "2000 0.6931472\n",
      "2500 0.6931472\n",
      "3000 0.6931472\n",
      "3500 0.6931472\n",
      "4000 0.6931472\n",
      "4500 0.6931472\n",
      "5000 0.6931472\n",
      "5500 0.6931472\n",
      "6000 0.6931472\n",
      "6500 0.6931472\n",
      "7000 0.6931472\n",
      "7500 0.6931472\n",
      "8000 0.6931472\n",
      "8500 0.6931472\n",
      "9000 0.6931472\n",
      "9500 0.6931472\n",
      "10000 0.6931472\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "# Hypothesis using sigmoid\n",
    "linear = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear, sigmoid)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nHypothesis: \", hypothesis.data.numpy(), \"\\nCorrect: \", predicted.numpy(), \"\\nAccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7434073\n",
      "500 0.69316936\n",
      "1000 0.69316494\n",
      "1500 0.693161\n",
      "2000 0.69315755\n",
      "2500 0.69315434\n",
      "3000 0.6931515\n",
      "3500 0.6931487\n",
      "4000 0.693146\n",
      "4500 0.69314325\n",
      "5000 0.69314075\n",
      "5500 0.693138\n",
      "6000 0.69313514\n",
      "6500 0.69313216\n",
      "7000 0.6931289\n",
      "7500 0.69312525\n",
      "8000 0.69312114\n",
      "8500 0.69311666\n",
      "9000 0.69311136\n",
      "9500 0.6931053\n",
      "10000 0.6930982\n",
      "\n",
      "Hypothesis:  [[0.503147  ]\n",
      " [0.50198627]\n",
      " [0.4982264 ]\n",
      " [0.49694133]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))\n",
    "\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    # cost/loss function\n",
    "    cost = -(Y * torch.log(hypothesis) + (1 - Y)\n",
    "             * torch.log(1 - hypothesis)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(step, cost.data.numpy())\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nHypothesis: \", hypothesis.data.numpy(), \"\\nCorrect: \", predicted.numpy(), \"\\nAccuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.612622142\n",
      "[Epoch:    2] cost = 0.343994766\n",
      "[Epoch:    3] cost = 0.307820737\n",
      "[Epoch:    4] cost = 0.291483611\n",
      "[Epoch:    5] cost = 0.28171739\n",
      "[Epoch:    6] cost = 0.274238229\n",
      "[Epoch:    7] cost = 0.269270748\n",
      "[Epoch:    8] cost = 0.265026271\n",
      "[Epoch:    9] cost = 0.261815339\n",
      "[Epoch:   10] cost = 0.258980632\n",
      "[Epoch:   11] cost = 0.256636649\n",
      "[Epoch:   12] cost = 0.254542679\n",
      "[Epoch:   13] cost = 0.252477646\n",
      "[Epoch:   14] cost = 0.251285702\n",
      "[Epoch:   15] cost = 0.24944666\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9102)\n",
      "Label:  tensor([0])\n",
      "Prediction:  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Linear(784, 10, bias=True)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.300109118\n",
      "[Epoch:    2] cost = 0.114167333\n",
      "[Epoch:    3] cost = 0.0748907253\n",
      "[Epoch:    4] cost = 0.0526830852\n",
      "[Epoch:    5] cost = 0.0397223905\n",
      "[Epoch:    6] cost = 0.0309094992\n",
      "[Epoch:    7] cost = 0.0244862568\n",
      "[Epoch:    8] cost = 0.018853141\n",
      "[Epoch:    9] cost = 0.0193276554\n",
      "[Epoch:   10] cost = 0.0159798954\n",
      "[Epoch:   11] cost = 0.0134580117\n",
      "[Epoch:   12] cost = 0.0114341313\n",
      "[Epoch:   13] cost = 0.0112195397\n",
      "[Epoch:   14] cost = 0.009181723\n",
      "[Epoch:   15] cost = 0.011222478\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9789)\n",
      "Label:  tensor([1])\n",
      "Prediction:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_XAVIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.249946088\n",
      "[Epoch:    2] cost = 0.0937100798\n",
      "[Epoch:    3] cost = 0.0605647229\n",
      "[Epoch:    4] cost = 0.0428153053\n",
      "[Epoch:    5] cost = 0.0320164412\n",
      "[Epoch:    6] cost = 0.024540849\n",
      "[Epoch:    7] cost = 0.0204218794\n",
      "[Epoch:    8] cost = 0.0187543128\n",
      "[Epoch:    9] cost = 0.0157501418\n",
      "[Epoch:   10] cost = 0.0162773505\n",
      "[Epoch:   11] cost = 0.0129219396\n",
      "[Epoch:   12] cost = 0.0120525798\n",
      "[Epoch:   13] cost = 0.00884464476\n",
      "[Epoch:   14] cost = 0.0116323736\n",
      "[Epoch:   15] cost = 0.00841329526\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9797)\n",
      "Label:  tensor([2])\n",
      "Prediction:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_NN_DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.210433677\n",
      "[Epoch:    2] cost = 0.0920023173\n",
      "[Epoch:    3] cost = 0.0666632578\n",
      "[Epoch:    4] cost = 0.050016962\n",
      "[Epoch:    5] cost = 0.0417470261\n",
      "[Epoch:    6] cost = 0.0316395909\n",
      "[Epoch:    7] cost = 0.0326432511\n",
      "[Epoch:    8] cost = 0.0296469349\n",
      "[Epoch:    9] cost = 0.0256317519\n",
      "[Epoch:   10] cost = 0.0221906193\n",
      "[Epoch:   11] cost = 0.0193258468\n",
      "[Epoch:   12] cost = 0.0197772924\n",
      "[Epoch:   13] cost = 0.0185328983\n",
      "[Epoch:   14] cost = 0.0162974503\n",
      "[Epoch:   15] cost = 0.0128309382\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9784)\n",
      "Label:  tensor([4])\n",
      "Prediction:  tensor([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu,\n",
    "                            linear2, relu,\n",
    "                            linear3, relu,\n",
    "                            linear4, relu,\n",
    "                            linear5)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST_DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.305789948\n",
      "[Epoch:    2] cost = 0.142377958\n",
      "[Epoch:    3] cost = 0.112523094\n",
      "[Epoch:    4] cost = 0.0973792151\n",
      "[Epoch:    5] cost = 0.081112504\n",
      "[Epoch:    6] cost = 0.0729576275\n",
      "[Epoch:    7] cost = 0.0689321458\n",
      "[Epoch:    8] cost = 0.0605747774\n",
      "[Epoch:    9] cost = 0.0594271719\n",
      "[Epoch:   10] cost = 0.0563444495\n",
      "[Epoch:   11] cost = 0.0522813238\n",
      "[Epoch:   12] cost = 0.0494968407\n",
      "[Epoch:   13] cost = 0.0473980233\n",
      "[Epoch:   14] cost = 0.044888556\n",
      "[Epoch:   15] cost = 0.0444605201\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9798)\n",
      "Label:  tensor([1])\n",
      "Prediction:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "# nn layers\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "# p is the probability of being dropped in PyTorch\n",
    "dropout = torch.nn.Dropout(p=1 - keep_prob)\n",
    "\n",
    "# xavier initializer\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)\n",
    "torch.nn.init.xavier_uniform(linear4.weight)\n",
    "torch.nn.init.xavier_uniform(linear5.weight)\n",
    "\n",
    "# model\n",
    "model = torch.nn.Sequential(linear1, relu, dropout,\n",
    "                            linear2, relu, dropout,\n",
    "                            linear3, relu, dropout,\n",
    "                            linear4, relu, dropout,\n",
    "                            linear5)\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        X = Variable(batch_xs.view(-1, 28 * 28))\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost.data[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "X_test = Variable(mnist_test.test_data.view(-1, 28 * 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "X_single_data = Variable(mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float())\n",
    "Y_single_data = Variable(mnist_test.test_labels[r:r + 1])\n",
    "\n",
    "print(\"Label: \", Y_single_data.data)\n",
    "single_prediction = model(X_single_data)\n",
    "print(\"Prediction: \", torch.max(single_prediction.data, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:51: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.222926348\n",
      "[Epoch:    2] cost = 0.0581157506\n",
      "[Epoch:    3] cost = 0.0437185243\n",
      "[Epoch:    4] cost = 0.0354796834\n",
      "[Epoch:    5] cost = 0.0293154102\n",
      "[Epoch:    6] cost = 0.025766952\n",
      "[Epoch:    7] cost = 0.0209184717\n",
      "[Epoch:    8] cost = 0.0183505304\n",
      "[Epoch:    9] cost = 0.0156245958\n",
      "[Epoch:   10] cost = 0.0135765392\n",
      "[Epoch:   11] cost = 0.0112947235\n",
      "[Epoch:   12] cost = 0.00936362334\n",
      "[Epoch:   13] cost = 0.00924605597\n",
      "[Epoch:   14] cost = 0.00702412333\n",
      "[Epoch:   15] cost = 0.00711970869\n",
      "Learning Finished!\n",
      "Accuracy: tensor(0.9876)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# CNN Model (2 conv layers)\n",
    "\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Final FC 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate CNN model\n",
    "model = CNN()\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        X = Variable(batch_xs)    # image is already size of (28x28), no reshape\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost.data / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "X_test = Variable(mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:109: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.286253005\n",
      "[Epoch:    2] cost = 0.0855991691\n",
      "[Epoch:    3] cost = 0.066839993\n",
      "[Epoch:    4] cost = 0.0572454929\n",
      "[Epoch:    5] cost = 0.0517041311\n",
      "[Epoch:    6] cost = 0.0457179099\n",
      "[Epoch:    7] cost = 0.0446779057\n",
      "[Epoch:    8] cost = 0.0425556488\n",
      "[Epoch:    9] cost = 0.038928315\n",
      "[Epoch:   10] cost = 0.0380497351\n",
      "[Epoch:   11] cost = 0.0371094234\n",
      "[Epoch:   12] cost = 0.0361424871\n",
      "[Epoch:   13] cost = 0.033690501\n",
      "[Epoch:   14] cost = 0.0327098556\n",
      "[Epoch:   15] cost = 0.0322406776\n",
      "Learning Finished!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-08960db16404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-08960db16404>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Flatten them for FC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 1GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "keep_prob = 0.7\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# CNN Model\n",
    "\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "        #    Conv      ->(?, 7, 7, 128)\n",
    "        #    Pool      ->(?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "\n",
    "        # L4 FC 4x4x128 inputs -> 625 outputs\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc1.weight)\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1 - keep_prob))\n",
    "        # L5 Final FC 625 inputs -> 10 outputs\n",
    "        self.fc2 = torch.nn.Linear(625, 10, bias=True)\n",
    "        torch.nn.init.xavier_uniform(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# instantiate CNN model\n",
    "model = CNN()\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(mnist_train) // batch_size\n",
    "\n",
    "    for i, (batch_xs, batch_ys) in enumerate(data_loader):\n",
    "        X = Variable(batch_xs)    # image is already size of (28x28), no reshape\n",
    "        Y = Variable(batch_ys)    # label is not one-hot encoded\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost.data / total_batch\n",
    "\n",
    "    print(\"[Epoch: {:>4}] cost = {:>.9}\".format(epoch + 1, avg_cost[0]))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "X_test = Variable(mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float())\n",
    "Y_test = Variable(mnist_test.test_labels)\n",
    "\n",
    "prediction = model(X_test)\n",
    "correct_prediction = (torch.max(prediction.data, 1)[1] == Y_test.data)\n",
    "accuracy = correct_prediction.float().mean()\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.645\n",
      "Predicted string:  oooooo\n",
      "epoch: 2, loss: 1.448\n",
      "Predicted string:  llllll\n",
      "epoch: 3, loss: 1.308\n",
      "Predicted string:  llllll\n",
      "epoch: 4, loss: 1.144\n",
      "Predicted string:  illlll\n",
      "epoch: 5, loss: 0.954\n",
      "Predicted string:  ilello\n",
      "epoch: 6, loss: 0.770\n",
      "Predicted string:  ihello\n",
      "epoch: 7, loss: 0.612\n",
      "Predicted string:  ihello\n",
      "epoch: 8, loss: 0.486\n",
      "Predicted string:  ihello\n",
      "epoch: 9, loss: 0.374\n",
      "Predicted string:  ihello\n",
      "epoch: 10, loss: 0.272\n",
      "Predicted string:  ihello\n",
      "epoch: 11, loss: 0.199\n",
      "Predicted string:  ihello\n",
      "epoch: 12, loss: 0.147\n",
      "Predicted string:  ihello\n",
      "epoch: 13, loss: 0.105\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.078\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.055\n",
      "Predicted string:  ihello\n",
      "Learning finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 15\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = torch.Tensor(x_one_hot)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "inputs = Variable(inputs)\n",
    "labels = Variable(labels)\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        # Set parameters for RNN block\n",
    "        # Note: batch_first=False by default.\n",
    "        # When true, inputs are (batch_size, sequence_length, input_dimension)\n",
    "        # instead of (sequence_length, batch_size, input_dimension)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        # Fully connected layer to obtain outputs corresponding to the number\n",
    "        # of classes\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            x.size(0), self.num_layers, self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (batch, num_layers * num_directions, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "\n",
    "        # Reshape output from (batch, seq_len, hidden_size) to (batch *\n",
    "        # seq_len, hidden_size)\n",
    "        out = out.view(-1, self.hidden_size)\n",
    "        # Return outputs applied to fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
